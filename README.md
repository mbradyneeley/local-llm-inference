# local-llm-inference

llama3/ needs cleaning
phi-3.5/ ready to go for cpu inference. On 1 cpu, runs 2.19 tok/sec
