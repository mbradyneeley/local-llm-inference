# local-llm-inference

![dalle_img](https://github.com/user-attachments/assets/d3b9982b-246f-463d-8f25-979375930864)


llama3/ needs cleaning
phi-3.5/ ready to go for cpu inference. On 1 cpu, runs 2.19 tok/sec
