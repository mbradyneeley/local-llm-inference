# local-llm-inference

![dalle_img](https://github.com/user-attachments/assets/d3b9982b-246f-463d-8f25-979375930864)


llama3/ needs cleaning<br>
phi-3.5/ ready to go for cpu inference. On 1 cpu, runs 2.19 tok/sec<br>
* For phi-3.5 inference on cpu just run the script `./phi-3.5/run_phi35moe_cpu.py` after updating the prompt<br>
