Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:08<03:56,  8.15s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:08<01:45,  3.78s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:09<01:04,  2.38s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:10<00:46,  1.77s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:11<00:35,  1.43s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:12<00:29,  1.24s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:20<01:21,  3.54s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:29<01:55,  5.24s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:39<02:24,  6.86s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:49<02:35,  7.80s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:59<02:38,  8.36s/it]Loading checkpoint shards:  40%|████      | 12/30 [01:08<02:36,  8.72s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [01:19<02:38,  9.30s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [01:29<02:33,  9.62s/it]Loading checkpoint shards:  50%|█████     | 15/30 [01:39<02:25,  9.67s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [01:44<01:53,  8.12s/it]Loading checkpoint shards:  60%|██████    | 18/30 [01:44<00:52,  4.40s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [01:44<00:26,  2.68s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [01:44<00:13,  1.74s/it]Loading checkpoint shards:  80%|████████  | 24/30 [01:44<00:07,  1.17s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [01:44<00:03,  1.23it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:45<00:01,  1.45it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:46<00:00,  1.59it/s]Loading checkpoint shards: 100%|██████████| 30/30 [01:46<00:00,  3.54s/it]
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
